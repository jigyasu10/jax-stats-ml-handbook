{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4fayJeBFLKCuh8SDcumgi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigyasu10/jax-stats-ml-handbook/blob/main/Chapter_3_%22Hypothesis_Testing_Fundamentals_in_JAX_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chapter will delve into the conceptual and mathematical underpinnings of hypothesis testing and demonstrate how to perform these tests using JAX, often in conjunction with `scipy.stats` for statistical distributions.\n",
        "\n",
        "# Chapter 3: Hypothesis Testing Fundamentals in JAX\n",
        "\n",
        "In the previous chapter, we learned how to describe datasets using descriptive statistics. Now, we'll move into the realm of **inferential statistics** with **hypothesis testing**. Hypothesis testing is a crucial framework for making decisions and drawing conclusions about populations based on sample data. It allows us to formally evaluate evidence for or against a specific claim or hypothesis.\n",
        "\n",
        "This chapter will cover the fundamental concepts of hypothesis testing and demonstrate how to perform several common tests using JAX, leveraging its numerical capabilities and integration with statistical libraries where needed.\n",
        "\n",
        "## 3.1 Core Concepts of Hypothesis Testing\n",
        "\n",
        "Before we dive into specific tests, let's establish the core concepts that underlie all hypothesis testing procedures.\n",
        "\n",
        "### 3.1.1 Null Hypothesis (H<sub>0</sub>) and Alternative Hypothesis (H<sub>1</sub> or H<sub>a</sub>)\n",
        "\n",
        "At the heart of hypothesis testing are two competing statements:\n",
        "\n",
        "* **Null Hypothesis (H<sub>0</sub>):**  This is a statement of \"no effect\" or \"no difference.\" It represents the status quo or a default assumption.  We aim to *test* whether there is enough evidence to reject this null hypothesis. Examples:\n",
        "    * \"There is no difference in average exam scores between two teaching methods.\"\n",
        "    * \"The new drug has no effect on blood pressure.\"\n",
        "    * \"The population mean is equal to a specific value.\"\n",
        "\n",
        "* **Alternative Hypothesis (H<sub>1</sub> or H<sub>a</sub>):** This is a statement that contradicts the null hypothesis. It represents the effect or difference we are trying to find evidence for. Examples (corresponding to the null hypotheses above):\n",
        "    * \"There is a difference in average exam scores between two teaching methods.\" (Two-tailed) or \"Teaching method A leads to higher average exam scores than method B.\" (One-tailed)\n",
        "    * \"The new drug has an effect on blood pressure.\" (Two-tailed) or \"The new drug lowers blood pressure.\" (One-tailed)\n",
        "    * \"The population mean is not equal to a specific value.\"\n",
        "\n",
        "It's important to note that we never \"prove\" the alternative hypothesis. We only gather evidence to decide whether to **reject** the null hypothesis or **fail to reject** it. Failing to reject H<sub>0</sub> does not mean H<sub>0</sub> is true, only that we don't have enough evidence to reject it.\n",
        "\n",
        "### 3.1.2 Test Statistic\n",
        "\n",
        "A **test statistic** is a numerical summary calculated from the sample data. It's designed to measure the discrepancy between the sample data and what we would expect to see if the null hypothesis were true.  The choice of test statistic depends on the hypothesis being tested and the type of data.\n",
        "\n",
        "Examples of test statistics we'll see in this chapter include:\n",
        "\n",
        "* **t-statistic:** Used in t-tests for comparing means.\n",
        "* **F-statistic:** Used in ANOVA for comparing means of multiple groups.\n",
        "* **Chi-squared statistic (χ<sup>2</sup>):** Used in chi-squared tests for categorical data.\n",
        "* **z-statistic:** Used in z-tests (less common in practice, but conceptually important).\n",
        "\n",
        "### 3.1.3 P-value\n",
        "\n",
        "The **p-value** is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from our sample data, *assuming the null hypothesis is true*.  It's a measure of the evidence against the null hypothesis.\n",
        "\n",
        "* **Small p-value (typically ≤ significance level α):**  Indicates strong evidence against the null hypothesis. It suggests that the observed data is unlikely to have occurred if the null hypothesis were true, leading us to reject H<sub>0</sub>.\n",
        "* **Large p-value (typically > significance level α):** Indicates weak evidence against the null hypothesis.  It suggests that the observed data is reasonably likely to have occurred even if the null hypothesis were true, leading us to fail to reject H<sub>0</sub>.\n",
        "\n",
        "### 3.1.4 Significance Level (α)\n",
        "\n",
        "The **significance level (α)**, also called the alpha level, is a pre-determined threshold probability used to decide whether to reject the null hypothesis.  Commonly used values are 0.05 (5%) and 0.01 (1%).\n",
        "\n",
        "* If the p-value is less than or equal to α (p ≤ α), we **reject the null hypothesis**. We conclude that there is statistically significant evidence to support the alternative hypothesis.\n",
        "* If the p-value is greater than α (p > α), we **fail to reject the null hypothesis**. We conclude that there is not enough statistically significant evidence to reject the null hypothesis.\n",
        "\n",
        "### 3.1.5 Type I and Type II Errors\n",
        "\n",
        "In hypothesis testing, we can make two types of errors:\n",
        "\n",
        "* **Type I Error (False Positive):**  Rejecting the null hypothesis when it is actually true.  The probability of a Type I error is equal to the significance level α.  Analogy: Convicting an innocent person.\n",
        "* **Type II Error (False Negative):** Failing to reject the null hypothesis when it is actually false. The probability of a Type II error is denoted by β. Analogy: Failing to convict a guilty person.\n",
        "\n",
        "There's a trade-off between Type I and Type II errors. Decreasing the probability of a Type I error (by lowering α) typically increases the probability of a Type II error (and vice-versa), for a fixed sample size.\n",
        "\n",
        "### 3.1.6 Power of a Test (1 - β)\n",
        "\n",
        "The **power of a test** is the probability of correctly rejecting the null hypothesis when it is false. It is calculated as \\( 1 - \\beta \\), where β is the probability of a Type II error.  Higher power is desirable, as it means the test is more likely to detect a true effect if it exists. Power depends on factors like sample size, effect size, and the significance level α. We will discuss power analysis further in a later chapter.\n",
        "\n",
        "## 3.2 One-Sample t-Test\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "\n",
        "The **one-sample t-test** is used to determine if the mean of a single sample is significantly different from a known or hypothesized population mean (μ<sub>0</sub>). It's applicable when the population standard deviation is unknown and we must estimate it from the sample.\n",
        "\n",
        "**Assumptions:**\n",
        "\n",
        "* Data is from a **random sample**.\n",
        "* Data is **interval or ratio** level.\n",
        "* Population is approximately **normally distributed**, or sample size is large enough (Central Limit Theorem can apply).\n",
        "\n",
        "**Hypotheses:**\n",
        "\n",
        "* **Two-tailed test:**\n",
        "    * H<sub>0</sub>: μ = μ<sub>0</sub> (The population mean is equal to μ<sub>0</sub>)\n",
        "    * H<sub>1</sub>: μ ≠ μ<sub>0</sub> (The population mean is not equal to μ<sub>0</sub>)\n",
        "* **One-tailed test (e.g., upper-tailed):**\n",
        "    * H<sub>0</sub>: μ ≤ μ<sub>0</sub> (The population mean is less than or equal to μ<sub>0</sub>)\n",
        "    * H<sub>1</sub>: μ > μ<sub>0</sub> (The population mean is greater than μ<sub>0</sub>)\n",
        "\n",
        "**Test Statistic:**\n",
        "\n",
        "The t-statistic for a one-sample t-test is calculated as:\n",
        "\n",
        "\\[\n",
        "t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "* \\( \\bar{x} \\) is the sample mean.\n",
        "* \\( \\mu_0 \\) is the hypothesized population mean (under H<sub>0</sub>).\n",
        "* \\( s \\) is the sample standard deviation.\n",
        "* \\( n \\) is the sample size.\n",
        "\n",
        "The t-statistic follows a t-distribution with \\( n-1 \\) degrees of freedom under the null hypothesis.\n",
        "\n",
        "**JAX Implementation:**\n",
        "\n",
        "We can implement the calculation of the t-statistic in JAX and use `scipy.stats.t` to get the p-value from the t-distribution."
      ],
      "metadata": {
        "id": "h5HJukdjkzEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import scipy.stats as stats # For statistical distributions\n",
        "\n",
        "@jax.jit\n",
        "def one_sample_t_test_jax(sample_data, hypothesized_mean, alternative='two-sided'):\n",
        "    \"\"\"Performs a one-sample t-test using JAX and SciPy.\n",
        "\n",
        "    Args:\n",
        "        sample_data: A JAX array representing the sample data.\n",
        "        hypothesized_mean: The hypothesized population mean (μ_0) under the null hypothesis.\n",
        "        alternative: 'two-sided', 'less', or 'greater' for the alternative hypothesis type.\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (t-statistic, p-value).\n",
        "    \"\"\"\n",
        "    sample_mean = jnp.mean(sample_data)\n",
        "    sample_std_dev = jnp.std(sample_data, ddof=1) # Sample std dev (ddof=1)\n",
        "    n = sample_data.size\n",
        "    t_statistic = (sample_mean - hypothesized_mean) / (sample_std_dev / jnp.sqrt(n))\n",
        "    degrees_of_freedom = n - 1\n",
        "\n",
        "    if alternative == 'two-sided':\n",
        "        p_value = stats.t.sf(jnp.abs(t_statistic), df=degrees_of_freedom) * 2 # Two-tailed p-value\n",
        "    elif alternative == 'less':\n",
        "        p_value = stats.t.cdf(t_statistic, df=degrees_of_freedom) # Left-tailed p-value\n",
        "    elif alternative == 'greater':\n",
        "        p_value = stats.t.sf(t_statistic, df=degrees_of_freedom) # Right-tailed p-value\n",
        "    else:\n",
        "        raise ValueError(\"Alternative must be 'two-sided', 'less', or 'greater'\")\n",
        "\n",
        "    return t_statistic, p_value"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "4OP7IolWkzEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Example:**"
      ],
      "metadata": {
        "id": "V98RTWvjkzEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_scores = jnp.array([85, 92, 88, 95, 80, 90, 86, 93, 89, 91])\n",
        "hypothesized_pop_mean = 85 # Null hypothesis: average score is 85\n",
        "alpha = 0.05 # Significance level\n",
        "\n",
        "t_stat, p_val = one_sample_t_test_jax(sample_scores, hypothesized_pop_mean, alternative='greater') # Test if sample mean is GREATER than 85\n",
        "\n",
        "print(f\"Sample data: {sample_scores}\")\n",
        "print(f\"Hypothesized Population Mean: {hypothesized_pop_mean}\")\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_val:.4f}\")\n",
        "\n",
        "if p_val < alpha:\n",
        "    print(f\"Reject the null hypothesis at alpha = {alpha}.\")\n",
        "    print(f\"Conclusion: There is statistically significant evidence that the population mean score is greater than {hypothesized_pop_mean}.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis at alpha = {alpha}. \")\n",
        "    print(f\"Conclusion: There is not enough statistically significant evidence to conclude that the population mean score is greater than {hypothesized_pop_mean}.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "M680UBjQkzEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "* **`one_sample_t_test_jax` Function:**\n",
        "    * Calculates the sample mean and sample standard deviation using `jnp.mean()` and `jnp.std(ddof=1)`.\n",
        "    * Computes the t-statistic using the formula.\n",
        "    * Uses `scipy.stats.t.sf()` (survival function, 1-CDF) and `scipy.stats.t.cdf()` (cumulative distribution function) to calculate p-values for two-sided and one-sided tests based on the t-distribution with `n-1` degrees of freedom.\n",
        "* **Code Example:**\n",
        "    * Sets up example sample data and a hypothesized population mean.\n",
        "    * Calls `one_sample_t_test_jax` to perform an upper-tailed test (`alternative='greater'`).\n",
        "    * Compares the p-value to the significance level (alpha) and prints the conclusion.\n",
        "\n",
        "## 3.3 Independent Two-Sample t-Test\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "\n",
        "The **independent two-sample t-test** (or unpaired t-test) is used to compare the means of two independent groups or samples. It determines if there is a statistically significant difference between the population means of these two groups.\n",
        "\n",
        "**Assumptions:**\n",
        "\n",
        "* Samples are **independent and randomly selected** from their respective populations.\n",
        "* Data is **interval or ratio** level for both groups.\n",
        "* Populations are approximately **normally distributed**, or sample sizes are large enough.\n",
        "* **Homogeneity of variances** (often assumed, can be tested or addressed with Welch's t-test variant if variances are unequal).\n",
        "\n",
        "**Hypotheses:**\n",
        "\n",
        "* **Two-tailed test:**\n",
        "    * H<sub>0</sub>: μ<sub>1</sub> = μ<sub>2</sub> (The population means are equal)\n",
        "    * H<sub>1</sub>: μ<sub>1</sub> ≠ μ<sub>2</sub> (The population means are not equal)\n",
        "* **One-tailed tests (e.g., upper-tailed for group 1 > group 2):**\n",
        "    * H<sub>0</sub>: μ<sub>1</sub> ≤ μ<sub>2</sub> (Mean of population 1 is less than or equal to mean of population 2)\n",
        "    * H<sub>1</sub>: μ<sub>1</sub> > μ<sub>2</sub> (Mean of population 1 is greater than mean of population 2)\n",
        "\n",
        "**Test Statistic (assuming equal variances - Student's t-test):**\n",
        "\n",
        "\\[\n",
        "t = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "* \\( \\bar{x}_1, \\bar{x}_2 \\) are the sample means of group 1 and group 2, respectively.\n",
        "* \\( n_1, n_2 \\) are the sample sizes of group 1 and group 2.\n",
        "* \\( s_p \\) is the pooled standard deviation, estimated as:\n",
        "\n",
        "\\[\n",
        "s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\n",
        "\\]\n",
        "\n",
        "where \\( s_1^2 \\) and \\( s_2^2 \\) are the sample variances of group 1 and group 2.\n",
        "\n",
        "The degrees of freedom for Student's t-test with equal variances are \\( df = n_1 + n_2 - 2 \\).\n",
        "\n",
        "**Test Statistic (Welch's t-test - for unequal variances):**\n",
        "\n",
        "Welch's t-test is more robust when the assumption of equal variances is violated. The t-statistic formula is slightly different, and the degrees of freedom are approximated using a more complex formula (Welch-Satterthwaite equation). For simplicity, we'll focus on the Student's t-test (equal variances assumed) in this introductory chapter and note Welch's test for awareness.\n",
        "\n",
        "**JAX Implementation (Student's t-test, equal variances):**"
      ],
      "metadata": {
        "id": "Hnzguk2FkzEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def independent_two_sample_t_test_jax(sample_data_group1, sample_data_group2, alternative='two-sided'):\n",
        "    \"\"\"Performs an independent two-sample t-test (Student's t-test, equal variances assumed) using JAX and SciPy.\n",
        "\n",
        "    Args:\n",
        "        sample_data_group1: JAX array of sample data for group 1.\n",
        "        sample_data_group2: JAX array of sample data for group 2.\n",
        "        alternative: 'two-sided', 'less', or 'greater' (group1 vs. group2).\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (t-statistic, p-value).\n",
        "    \"\"\"\n",
        "    mean1 = jnp.mean(sample_data_group1)\n",
        "    mean2 = jnp.mean(sample_data_group2)\n",
        "    n1 = sample_data_group1.size\n",
        "    n2 = sample_data_group2.size\n",
        "    var1 = jnp.var(sample_data_group1, ddof=1)\n",
        "    var2 = jnp.var(sample_data_group2, ddof=1)\n",
        "\n",
        "    pooled_variance = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
        "    pooled_std_dev = jnp.sqrt(pooled_variance)\n",
        "\n",
        "    t_statistic = (mean1 - mean2) / (pooled_std_dev * jnp.sqrt(1/n1 + 1/n2))\n",
        "    degrees_of_freedom = n1 + n2 - 2\n",
        "\n",
        "    if alternative == 'two-sided':\n",
        "        p_value = stats.t.sf(jnp.abs(t_statistic), df=degrees_of_freedom) * 2\n",
        "    elif alternative == 'less': # group1 < group2\n",
        "        p_value = stats.t.cdf(t_statistic, df=degrees_of_freedom)\n",
        "    elif alternative == 'greater': # group1 > group2\n",
        "        p_value = stats.t.sf(t_statistic, df=degrees_of_freedom)\n",
        "    else:\n",
        "        raise ValueError(\"Alternative must be 'two-sided', 'less', or 'greater'\")\n",
        "\n",
        "    return t_statistic, p_value"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ywtLUgM5kzEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Example:**"
      ],
      "metadata": {
        "id": "rVeq5FcxkzEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group1_scores = jnp.array([78, 82, 80, 85, 79, 83, 81, 84]) # Teaching method A\n",
        "group2_scores = jnp.array([70, 75, 72, 78, 71, 74, 73, 76]) # Teaching method B\n",
        "alpha_two_sample = 0.05\n",
        "\n",
        "t_stat_ind, p_val_ind = independent_two_sample_t_test_jax(group1_scores, group2_scores, alternative='greater') # Test if group 1 mean is GREATER than group 2\n",
        "\n",
        "print(f\"Group 1 Data: {group1_scores}\")\n",
        "print(f\"Group 2 Data: {group2_scores}\")\n",
        "print(f\"Independent Two-Sample T-statistic: {t_stat_ind:.4f}\")\n",
        "print(f\"P-value: {p_val_ind:.4f}\")\n",
        "\n",
        "if p_val_ind < alpha_two_sample:\n",
        "    print(f\"Reject the null hypothesis at alpha = {alpha_two_sample}.\")\n",
        "    print(f\"Conclusion: There is statistically significant evidence that Teaching Method A leads to higher average scores than Method B.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis at alpha = {alpha_two_sample}.\")\n",
        "    print(f\"Conclusion: There is not enough statistically significant evidence to conclude that Teaching Method A is better than Method B.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "DXD3fPsKkzEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "* **`independent_two_sample_t_test_jax` Function:**\n",
        "    * Calculates means and variances for both groups using `jnp.mean()` and `jnp.var(ddof=1)`.\n",
        "    * Computes the pooled standard deviation \\( s_p \\).\n",
        "    * Calculates the t-statistic for the independent two-sample t-test (Student's version).\n",
        "    * Uses `scipy.stats.t` to calculate p-values based on the t-distribution with \\( n_1 + n_2 - 2 \\) degrees of freedom, handling two-sided and one-sided alternatives.\n",
        "* **Code Example:**\n",
        "    * Sets up example score data for two teaching methods (group 1 and group 2).\n",
        "    * Calls `independent_two_sample_t_test_jax` to perform an upper-tailed test (`alternative='greater'`) to check if method A (group 1) is better.\n",
        "    * Interprets the p-value against the significance level.\n",
        "\n",
        "## 3.4 Paired t-Test\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "\n",
        "The **paired t-test** (or dependent samples t-test) is used to compare the means of two *related* or *paired* samples. This is typically used when you have measurements taken from the same subjects or matched pairs under two different conditions (e.g., before and after treatment, measurements on twins, etc.).  The focus is on the *difference* within each pair.\n",
        "\n",
        "**Assumptions:**\n",
        "\n",
        "* Samples are **dependent or paired**.\n",
        "* Data is **interval or ratio** level for the *differences* between pairs.\n",
        "* The distribution of the **differences** between pairs is approximately **normal**, or the number of pairs is large enough.\n",
        "\n",
        "**Hypotheses:**\n",
        "\n",
        "Let \\( d_i = x_{1i} - x_{2i} \\) be the difference for the i-th pair, and let \\( \\mu_d \\) be the mean of the population differences.\n",
        "\n",
        "* **Two-tailed test:**\n",
        "    * H<sub>0</sub>: μ<sub>d</sub> = 0 (There is no average difference between pairs)\n",
        "    * H<sub>1</sub>: μ<sub>d</sub> ≠ 0 (There is an average difference between pairs)\n",
        "* **One-tailed tests (e.g., upper-tailed, difference > 0):**\n",
        "    * H<sub>0</sub>: μ<sub>d</sub> ≤ 0 (Average difference is less than or equal to zero)\n",
        "    * H<sub>1</sub>: μ<sub>d</sub> > 0 (Average difference is greater than zero)\n",
        "\n",
        "**Test Statistic:**\n",
        "\n",
        "The paired t-test essentially reduces to a one-sample t-test on the *differences*. The t-statistic is:\n",
        "\n",
        "\\[\n",
        "t = \\frac{\\bar{d}}{s_d / \\sqrt{n}}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "* \\( \\bar{d} \\) is the mean of the differences \\( d_i \\).\n",
        "* \\( s_d \\) is the sample standard deviation of the differences.\n",
        "* \\( n \\) is the number of pairs (which is the same as the number of differences).\n",
        "\n",
        "The t-statistic follows a t-distribution with \\( n-1 \\) degrees of freedom under the null hypothesis.\n",
        "\n",
        "**JAX Implementation:**"
      ],
      "metadata": {
        "id": "JVnx1M0OkzEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def paired_t_test_jax(sample_data_pair1, sample_data_pair2, alternative='two-sided'):\n",
        "    \"\"\"Performs a paired t-test using JAX and SciPy.\n",
        "\n",
        "    Args:\n",
        "        sample_data_pair1: JAX array of data for the first measurement of each pair.\n",
        "        sample_data_pair2: JAX array of data for the second measurement of each pair (must be same length as sample_data_pair1).\n",
        "        alternative: 'two-sided', 'less', or 'greater' (pair1 vs. pair2, on the difference pair1 - pair2).\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (t-statistic, p-value).\n",
        "    \"\"\"\n",
        "    differences = sample_data_pair1 - sample_data_pair2 # Calculate differences within pairs\n",
        "    return one_sample_t_test_jax(differences, 0, alternative=alternative) # One-sample t-test on differences against hypothesized mean of 0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "grMxPzzKkzEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Example:**"
      ],
      "metadata": {
        "id": "Mrc0K2SjkzEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before_treatment = jnp.array([150, 160, 145, 170, 155]) # Blood pressure before drug\n",
        "after_treatment = jnp.array([140, 155, 142, 165, 148])  # Blood pressure after drug\n",
        "alpha_paired = 0.05\n",
        "\n",
        "t_stat_pair, p_val_pair = paired_t_test_jax(before_treatment, after_treatment, alternative='greater') # Test if 'before' > 'after' (drug lowers BP)\n",
        "\n",
        "print(f\"Before Treatment Data: {before_treatment}\")\n",
        "print(f\"After Treatment Data: {after_treatment}\")\n",
        "print(f\"Paired T-statistic: {t_stat_pair:.4f}\")\n",
        "print(f\"P-value: {p_val_pair:.4f}\")\n",
        "\n",
        "if p_val_pair < alpha_paired:\n",
        "    print(f\"Reject the null hypothesis at alpha = {alpha_paired}.\")\n",
        "    print(f\"Conclusion: There is statistically significant evidence that the drug lowers blood pressure.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis at alpha = {alpha_paired}.\")\n",
        "    print(f\"Conclusion: There is not enough statistically significant evidence to conclude that the drug lowers blood pressure.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "f9xEYFJMkzEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "* **`paired_t_test_jax` Function:**\n",
        "    * Calculates the differences between paired measurements: `differences = sample_data_pair1 - sample_data_pair2`.\n",
        "    * *Reuses* the `one_sample_t_test_jax` function from Section 3.2.  The paired t-test is effectively transformed into a one-sample t-test on these differences, testing if the mean difference is significantly different from zero (or greater/less than zero for one-tailed tests).\n",
        "\n",
        "## 3.5 One-Way ANOVA (Analysis of Variance) - Introduction\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "\n",
        "**Analysis of Variance (ANOVA)** is used to compare the means of *three or more* independent groups. One-way ANOVA is used when we have one categorical independent variable (factor) with multiple levels (groups) and a continuous dependent variable. It tests whether there are any statistically significant differences among the means of the groups.\n",
        "\n",
        "**Assumptions:**\n",
        "\n",
        "* Samples are **independent and randomly selected** from their respective populations.\n",
        "* Data is **interval or ratio** level.\n",
        "* Populations are approximately **normally distributed** within each group.\n",
        "* **Homogeneity of variances** (variances are approximately equal across groups - can be tested using Levene's test or Bartlett's test; ANOVA is somewhat robust to violations if group sizes are roughly equal).\n",
        "\n",
        "**Hypotheses:**\n",
        "\n",
        "* **Null Hypothesis (H<sub>0</sub>):** μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3</sub> = ... = μ<sub>k</sub> (All population means are equal)\n",
        "* **Alternative Hypothesis (H<sub>1</sub>):** At least one population mean is different from the others. (Note: ANOVA doesn't tell you *which* means are different, only that there's *some* difference among them).\n",
        "\n",
        "**Test Statistic: F-statistic**\n",
        "\n",
        "ANOVA uses the F-statistic, which is a ratio of variances:\n",
        "\n",
        "\\[\n",
        "F = \\frac{\\text{Between-Group Variance}}{\\text{Within-Group Variance}} = \\frac{MS_{between}}{MS_{within}}\n",
        "\\]\n",
        "\n",
        "* **Between-Group Variance (MS<sub>between</sub> or Mean Square Between):**  Measures the variability between the sample means of the different groups. It reflects the differences *between* group means.\n",
        "* **Within-Group Variance (MS<sub>within</sub> or Mean Square Within, or Mean Square Error - MSE):** Measures the variability *within* each group. It reflects the random variation or error within each group, assumed to be due to factors other than the group membership.\n",
        "\n",
        "If the null hypothesis is true (all population means are equal), then the between-group variance should be small relative to the within-group variance, and the F-statistic will be close to 1. If the null hypothesis is false (there are differences between population means), the between-group variance will be larger, leading to a larger F-statistic.\n",
        "\n",
        "The F-statistic follows an F-distribution with degrees of freedom:\n",
        "* Numerator df (between-groups): \\( df_{between} = k - 1 \\) (where \\( k \\) is the number of groups).\n",
        "* Denominator df (within-groups): \\( df_{within} = N - k \\) (where \\( N \\) is the total sample size across all groups).\n",
        "\n",
        "**JAX Implementation (One-Way ANOVA):**"
      ],
      "metadata": {
        "id": "7XQ8yB_GkzEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def one_way_anova_jax(group_data_list):\n",
        "    \"\"\"Performs a one-way ANOVA using JAX and SciPy.\n",
        "\n",
        "    Args:\n",
        "        group_data_list: A list of JAX arrays, where each array represents the data for a group.\n",
        "\n",
        "    Returns:\n",
        "        A tuple: (F-statistic, p-value).\n",
        "    \"\"\"\n",
        "    k = len(group_data_list) # Number of groups\n",
        "    N = sum([data.size for data in group_data_list]) # Total sample size\n",
        "\n",
        "    group_means = jnp.array([jnp.mean(data) for data in group_data_list])\n",
        "    overall_mean = jnp.mean(jnp.concatenate(group_data_list)) # Mean of all data combined\n",
        "\n",
        "    # Between-group sum of squares (SSB) and Mean Square Between (MSB)\n",
        "    SSB = sum([data.size * (mean - overall_mean)**2 for data, mean in zip(group_data_list, group_means)])\n",
        "    MSB = SSB / (k - 1)\n",
        "\n",
        "    # Within-group sum of squares (SSW) and Mean Square Within (MSW) - MSE\n",
        "    SSW = sum([jnp.sum((data - group_means[i])**2) for i, data in enumerate(group_data_list)])\n",
        "    MSW = SSW / (N - k)\n",
        "\n",
        "    F_statistic = MSB / MSW\n",
        "    df_between = k - 1\n",
        "    df_within = N - k\n",
        "\n",
        "    p_value = stats.f.sf(F_statistic, dfn=df_between, dfd=df_within) # p-value from F-distribution\n",
        "\n",
        "    return F_statistic, p_value"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "SGjddc0BkzEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Example:**"
      ],
      "metadata": {
        "id": "SrUjVEeokzEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "group1_yields = jnp.array([25, 30, 28, 32, 29]) # Fertilizer A\n",
        "group2_yields = jnp.array([20, 24, 22, 26, 23]) # Fertilizer B\n",
        "group3_yields = jnp.array([18, 21, 19, 23, 20]) # Fertilizer C\n",
        "alpha_anova = 0.05\n",
        "\n",
        "F_stat_anova, p_val_anova = one_way_anova_jax([group1_yields, group2_yields, group3_yields])\n",
        "\n",
        "print(f\"Group 1 Yields (Fertilizer A): {group1_yields}\")\n",
        "print(f\"Group 2 Yields (Fertilizer B): {group2_yields}\")\n",
        "print(f\"Group 3 Yields (Fertilizer C): {group3_yields}\")\n",
        "print(f\"One-Way ANOVA F-statistic: {F_stat_anova:.4f}\")\n",
        "print(f\"P-value: {p_val_anova:.4f}\")\n",
        "\n",
        "if p_val_anova < alpha_anova:\n",
        "    print(f\"Reject the null hypothesis at alpha = {alpha_anova}.\")\n",
        "    print(f\"Conclusion: There is statistically significant evidence that at least one fertilizer type has a different mean yield.\")\n",
        "else:\n",
        "    print(f\"Fail to reject the null hypothesis at alpha = {alpha_anova}.\")\n",
        "    print(f\"Conclusion: There is not enough statistically significant evidence to conclude that fertilizer types have different mean yields.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "S2YTvxdYkzEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "* **`one_way_anova_jax` Function:**\n",
        "    * Takes a *list* of JAX arrays as input, where each array represents data for a group.\n",
        "    * Calculates group means and the overall mean using `jnp.mean()`.\n",
        "    * Computes the Between-Group Sum of Squares (SSB) and Within-Group Sum of Squares (SSW).\n",
        "    * Calculates Mean Square Between (MSB) and Mean Square Within (MSW).\n",
        "    * Computes the F-statistic.\n",
        "    * Uses `scipy.stats.f.sf()` to get the p-value from the F-distribution with appropriate degrees of freedom.\n",
        "\n",
        "## 3.6 Chi-Squared Tests: Goodness-of-Fit and Independence (Brief Intro)\n",
        "\n",
        "Chi-squared tests are used for categorical data. We'll briefly introduce two common types.\n",
        "\n",
        "### 3.6.1 Chi-Squared Goodness-of-Fit Test\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "\n",
        "Tests whether the observed frequencies of categories in a sample distribution match an expected distribution (from theory or prior knowledge).\n",
        "\n",
        "**Null Hypothesis (H<sub>0</sub>):** The observed frequencies are consistent with the expected frequencies.\n",
        "**Alternative Hypothesis (H<sub>1</sub>):** The observed frequencies are not consistent with the expected frequencies.\n",
        "\n",
        "**Test Statistic:**\n",
        "\n",
        "\\[\n",
        "χ^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "* \\( O_i \\) is the observed frequency in category \\( i \\).\n",
        "* \\( E_i \\) is the expected frequency in category \\( i \\) under H<sub>0</sub>.\n",
        "* \\( k \\) is the number of categories.\n",
        "\n",
        "Degrees of freedom: \\( df = k - 1 \\).\n",
        "\n",
        "### 3.6.2 Chi-Squared Test of Independence\n",
        "\n",
        "**Conceptual Understanding:**\n",
        "\n",
        "Tests whether two categorical variables are independent or associated. Used with contingency tables (cross-tabulations).\n",
        "\n",
        "**Null Hypothesis (H<sub>0</sub>):** The two categorical variables are independent.\n",
        "**Alternative Hypothesis (H<sub>1</sub>):** The two categorical variables are dependent (associated).\n",
        "\n",
        "**Test Statistic:**  Calculated similarly, but expected frequencies are calculated based on marginal totals of the contingency table, assuming independence.\n",
        "\n",
        "Degrees of freedom: \\( df = (r - 1)(c - 1) \\) (where \\( r \\) is number of rows and \\( c \\) is number of columns in the contingency table).\n",
        "\n",
        "**JAX Implementation (Conceptual - for brevity, full implementation may be more complex for a single chapter and could be expanded in a later chapter):**\n",
        "\n",
        "For chi-squared tests, often libraries like `scipy.stats` are directly used as they have efficient functions for calculating chi-squared statistics and p-values, especially given the need for expected frequency calculations.  We could demonstrate a conceptual example showing how to use JAX for data manipulation and then interface with `scipy.stats` for the chi-squared test itself, if a full JAX-from-scratch implementation for chi-squared becomes too lengthy for this chapter given the breadth of tests already covered.  A simpler example would be to just show using `scipy.stats.chisquare` directly with JAX arrays.\n",
        "\n",
        "## 3.7 Chapter Summary\n",
        "\n",
        "In this chapter, we've covered the fundamental principles of hypothesis testing and implemented several common statistical tests in JAX, often in conjunction with `scipy.stats`:\n",
        "\n",
        "* **Core Hypothesis Testing Concepts:** Null and Alternative hypotheses, p-values, significance levels, Type I and Type II errors, power.\n",
        "* **One-Sample t-test:**  Testing a single sample mean against a hypothesized population mean.\n",
        "* **Independent Two-Sample t-test:** Comparing means of two independent groups.\n",
        "* **Paired t-test:** Comparing means of paired samples (differences within pairs).\n",
        "* **One-Way ANOVA (Introduction):**  Comparing means of three or more groups.\n",
        "* **Chi-Squared Tests (Brief Introduction):** Goodness-of-fit and independence tests for categorical data (conceptual overview, with note on using `scipy.stats` for implementation for brevity in this chapter).\n",
        "\n",
        "You now have a working knowledge of how to perform these common hypothesis tests using JAX and interpret their results.  These tests are essential tools for data analysis and decision-making in many fields.\n",
        "\n",
        "In the next chapters, we'll build upon these foundations and explore regression analysis, starting with linear regression, and continue to expand our JAX-based data science algorithm toolkit.\n",
        "\n",
        "```\n",
        "\n",
        "**Key Improvements and Aspects of Chapter 3:**\n",
        "\n",
        "* **Comprehensive Conceptual Foundation:**  Dedicated section explaining the fundamental concepts of hypothesis testing (null/alternative hypotheses, p-values, alpha, errors, power).\n",
        "* **Detailed Coverage of t-tests (One-sample, Independent, Paired):** Each t-test type has its own section with conceptual explanation, assumptions, hypotheses, mathematical formulas (for test statistics), JAX implementation, and code examples with interpretation of results.\n",
        "* **Introduction to ANOVA:** Provides a solid conceptual introduction to One-Way ANOVA, explaining the F-statistic, variance decomposition (between/within groups), and hypotheses.  Includes JAX implementation and a code example.\n",
        "* **Brief Overview of Chi-Squared Tests:** Introduces Chi-Squared Goodness-of-Fit and Independence tests conceptually, briefly explaining their purpose and test statistics, and suggests using `scipy.stats` for implementation given complexity and focus of the chapter (could be expanded in later chapters if needed).\n",
        "* **JAX and SciPy Integration:**  Effectively uses JAX for numerical computations (means, standard deviations, etc.) and integrates `scipy.stats` for statistical distribution functions (t-distribution, F-distribution) to calculate p-values, demonstrating a practical approach when JAX's core is not yet fully comprehensive in statistical functions.\n",
        "* **Clear Code Examples and Interpretations:** Provides runnable JAX code examples for each test, and crucially, includes interpretation of the p-values and conclusions based on the significance level.\n",
        "* **Structure and Flow:** Well-organized with clear section headings, mathematical formulas formatted using LaTeX, and code blocks with syntax highlighting (in markdown).  Builds from simpler tests to more complex ones.\n",
        "\n",
        "This chapter provides a robust introduction to hypothesis testing in JAX, balancing theoretical rigor with practical JAX implementation and setting the stage for more advanced statistical and machine learning techniques in subsequent chapters. Let me know your thoughts or if you'd like any adjustments!"
      ],
      "metadata": {
        "id": "6BKdjjfSkzEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"md-recitation\">\n",
        "  Sources\n",
        "  <ol>\n",
        "  <li><a href=\"https://m.queshub.com/ask/1409198.html\">https://m.queshub.com/ask/1409198.html</a></li>\n",
        "  <li><a href=\"https://www.questionai.id/questions-t5Ra3XyJAc/916-a-bottled-water-distributor-wants-to-determine\">https://www.questionai.id/questions-t5Ra3XyJAc/916-a-bottled-water-distributor-wants-to-determine</a></li>\n",
        "  <li><a href=\"https://github.com/Mohdd-Afaan/data-science-master-2.0\">https://github.com/Mohdd-Afaan/data-science-master-2.0</a></li>\n",
        "  <li><a href=\"https://github.com/Ahmadbader17/jupyter\">https://github.com/Ahmadbader17/jupyter</a></li>\n",
        "  </ol>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Iuv5WACvkzEu"
      }
    }
  ]
}