{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPH/4UPelGziRsUnJHjOXle",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigyasu10/jax-stats-ml-handbook/blob/main/Chapter_1_Introduction_to_JAX_for_Data_Science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://)\n",
        "# Chapter 1: Introduction to JAX for Data Science\n",
        "\n",
        "Welcome to the exciting world of JAX for data science!  If you're picking up this book, you likely have an interest in understanding the fundamental algorithms that power data science and a desire to implement them efficiently.  You've come to the right place.\n",
        "\n",
        "In today's data-driven world, the demand for scalable and high-performance data analysis is ever-increasing.  We're dealing with larger datasets, more complex models, and a need for faster iteration in research and development.  Traditional numerical computing tools, while powerful, can sometimes become bottlenecks when we push the boundaries of computational intensity.\n",
        "\n",
        "Enter **JAX**.\n",
        "\n",
        "This chapter serves as your launchpad into using JAX, a powerful numerical computation library that's rapidly gaining traction in the data science and machine learning communities.  We'll explore what JAX is, why it's particularly well-suited for data science algorithms, and set up your environment to start experimenting.\n",
        "\n",
        "## 1.1 What is JAX? - NumPy on Steroids (and More!)\n",
        "\n",
        "At its core, JAX can be thought of as **NumPy on steroids**.  If you're already familiar with Python's NumPy library, you'll find yourself immediately comfortable with JAX's array manipulation capabilities. JAX provides a near-identical API to NumPy, meaning much of your existing NumPy code can often be ported to JAX with minimal changes.\n",
        "\n",
        "However, JAX is much more than just a NumPy clone.  It brings to the table a set of powerful features that are game-changers for numerical computing, especially in data science and machine learning:\n",
        "\n",
        "* **Automatic Differentiation:** JAX can automatically compute gradients (derivatives) of your Python and NumPy code. This is absolutely crucial for implementing gradient-based optimization algorithms, which are at the heart of many machine learning and statistical methods.\n",
        "* **Just-In-Time (JIT) Compilation:** JAX can compile your Python functions down to optimized machine code, often leading to significant speedups.  Think of it as turning your dynamic Python code into the efficiency of compiled languages like C or Fortran, without leaving the Python ecosystem.\n",
        "* **Automatic Vectorization and Parallelization:** JAX can automatically vectorize operations, allowing them to run efficiently on CPUs, GPUs, and TPUs.  This means your code can seamlessly scale to utilize powerful hardware accelerators.\n",
        "* **Composable Transformations:** JAX provides a set of composable function transformations (`jit`, `grad`, `vmap`, `pmap`, etc.) that can be combined in powerful ways to manipulate and optimize your numerical computations.\n",
        "\n",
        "In essence, JAX empowers you to write numerical code that is:\n",
        "\n",
        "* **Fast:**  Through compilation and hardware acceleration.\n",
        "* **Flexible:**  Retaining the dynamic nature and expressiveness of Python.\n",
        "* **Scalable:**  Easily leveraging GPUs and TPUs for larger workloads.\n",
        "* **Differentiable:**  Making gradient-based algorithms straightforward to implement.\n",
        "\n",
        "## 1.2 Why JAX for Data Science Algorithms?\n",
        "\n",
        "Why is JAX the right tool for a book focused on common data science algorithms?  Several reasons stand out:\n",
        "\n",
        "* **Algorithm Implementation:** Many algorithms in statistics, econometrics, experimentation, and machine learning rely heavily on numerical computation, linear algebra, and optimization. JAX excels in all of these areas.  Its automatic differentiation and compilation capabilities make implementing gradient-based algorithms (like gradient descent for regression, neural networks, etc.) remarkably simple and efficient.\n",
        "\n",
        "* **Performance and Scalability:**  As datasets grow, the computational demands of even \"common\" algorithms increase. JAX's ability to run on GPUs and TPUs ensures that your implementations remain performant even with larger datasets. This book isn't just about *understanding* the algorithms, but also about implementing them in a way that can handle real-world scale.\n",
        "\n",
        "* **Clarity and Code Conciseness:** While performance is crucial, so is code readability. JAXâ€™s NumPy-like API makes the code for implementing algorithms relatively clear and concise, especially when compared to lower-level languages or more verbose numerical libraries.  We can focus on the logic of the algorithms without getting bogged down in complex low-level implementation details.\n",
        "\n",
        "* **Modern and Evolving Ecosystem:** JAX is actively developed and has a growing ecosystem of libraries built around it, including libraries for optimization (Optax), neural networks (Flax, Haiku), and more.  Learning JAX puts you at the forefront of modern numerical computing in Python.\n",
        "\n",
        "* **Bridging Theory and Practice:** This book aims to bridge the gap between the theoretical understanding of data science algorithms and their practical implementation. JAX provides the ideal platform to achieve this, allowing you to translate mathematical formulations directly into efficient and executable code.\n",
        "\n",
        "## 1.3 Core JAX Concepts We'll Use\n",
        "\n",
        "Throughout this book, we'll leverage several core JAX concepts extensively. Let's briefly introduce the most important ones now:\n",
        "\n",
        "### 1.3.1 `jax.numpy` - Your Familiar Friend\n",
        "\n",
        "As mentioned, `jax.numpy` is your entry point into JAX if you are familiar with NumPy.  It provides almost the same API as standard NumPy. You can perform array operations, linear algebra, random number generation, and much more, all using `jax.numpy`."
      ],
      "metadata": {
        "id": "S-a0EU0pBJGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# Create a JAX array\n",
        "x = jnp.array([1.0, 2.0, 3.0])\n",
        "print(f\"JAX array: {x}\")\n",
        "print(f\"Array type: {type(x)}\") # Note: jaxlib.xla_extension.DeviceArray\n",
        "\n",
        "# Perform NumPy-like operations\n",
        "y = jnp.sin(x) + jnp.exp(x)\n",
        "print(f\"Result of operations: {y}\")\n",
        "````\n",
        "\n",
        "Notice that while the code looks very similar to NumPy, the array type is `jaxlib.xla_extension.DeviceArray`. This signifies that these arrays can be readily processed by JAX's transformations and potentially live on accelerators like GPUs or TPUs.\n",
        "\n",
        "### 1.3.2 `jax.jit` - Speeding Things Up with Compilation\n",
        "\n",
        "`jax.jit` is a crucial function in JAX. It stands for \"Just-In-Time compilation\".  When you decorate a function with `@jax.jit`, JAX will compile that function the first time it's called.  Subsequent calls to the same function with arguments of the same type and shape will then execute the compiled, optimized version, often resulting in significant speed improvements.\n",
        "\n",
        "```python\n",
        "import jax\n",
        "import time\n",
        "\n",
        "def slow_function(x):\n",
        "    result = 0.0\n",
        "    for _ in range(10000): # Simulate some computation\n",
        "        result += jnp.sin(x) * jnp.cos(x)\n",
        "    return result\n",
        "\n",
        "@jax.jit\n",
        "def fast_function(x):\n",
        "    result = 0.0\n",
        "    for _ in range(10000):\n",
        "        result += jnp.sin(x) * jnp.cos(x)\n",
        "    return result\n",
        "\n",
        "x_val = jnp.array(0.5)\n",
        "\n",
        "# Time the slow function\n",
        "start_time = time.time()\n",
        "slow_result = slow_function(x_val)\n",
        "slow_time = time.time() - start_time\n",
        "print(f\"Slow function result: {slow_result:.4f}, Time: {slow_time:.4f} seconds\")\n",
        "\n",
        "# Time the fast (jitted) function - First call includes compilation time\n",
        "start_time = time.time()\n",
        "fast_result = fast_function(x_val)\n",
        "compile_time = time.time() - start_time\n",
        "print(f\"Fast function (first call) result: {fast_result:.4f}, Time (compile + run): {compile_time:.4f} seconds\")\n",
        "\n",
        "# Time the fast (jitted) function - Subsequent calls are much faster\n",
        "start_time = time.time()\n",
        "fast_result = fast_function(x_val)\n",
        "fast_time = time.time() - start_time\n",
        "print(f\"Fast function (second call) result: {fast_result:.4f}, Time: {fast_time:.4f} seconds\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "yWg4I6qfBJGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll often see a significant speed difference, especially for computationally intensive functions, after JIT compilation. The first call to a jitted function might take a bit longer due to compilation, but subsequent calls are typically much faster.\n",
        "\n",
        "### 1.3.3 `jax.grad` - Automatic Differentiation at Your Fingertips\n",
        "\n",
        "`jax.grad` is where the magic of automatic differentiation happens.  It's a function that takes another function as input and returns a *new* function that computes the gradient of the original function. This is incredibly powerful for implementing optimization algorithms."
      ],
      "metadata": {
        "id": "Cm7ORZXFBJGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def square_function(x):\n",
        "    return x**2\n",
        "\n",
        "# Get the gradient function\n",
        "grad_square = jax.grad(square_function)\n",
        "\n",
        "x_value = 3.0\n",
        "gradient_at_x = grad_square(x_value)\n",
        "print(f\"Function value at x={x_value}: {square_function(x_value)}\")\n",
        "print(f\"Gradient at x={x_value}: {gradient_at_x}\") # Expected gradient: 2*x = 6"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "eEbygB5-BJGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX can compute gradients of complex functions, including those involving loops, control flow, and NumPy operations, with ease.  We'll heavily rely on `jax.grad` when implementing algorithms that require gradient-based optimization.\n",
        "\n",
        "### 1.3.4 `jax.vmap` - Automatic Vectorization\n",
        "\n",
        "`jax.vmap` is for automatic vectorization.  It allows you to automatically vectorize a function that is written to operate on single data points so that it can operate on batches of data points efficiently. This is crucial for processing datasets in parallel."
      ],
      "metadata": {
        "id": "-N3C5hW2BJGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def elementwise_square(x):\n",
        "    return x**2\n",
        "\n",
        "# Original function operates on a single number\n",
        "single_value = 2.0\n",
        "print(f\"Elementwise square of {single_value}: {elementwise_square(single_value)}\")\n",
        "\n",
        "# Use vmap to vectorize it to operate on an array\n",
        "batch_values = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
        "batched_square = jax.vmap(elementwise_square) # vmap returns a vectorized version of the function\n",
        "batched_results = batched_square(batch_values)\n",
        "print(f\"Batched squares: {batched_results}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "gvs5hnbdBJGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`jax.vmap` can dramatically simplify your code when you need to apply a function element-wise across arrays or batches of data, and it often leads to performance improvements by enabling vectorized operations.\n",
        "\n",
        "## 1.4 Setting up JAX\n",
        "\n",
        "Getting started with JAX is straightforward.  If you have Python installed, you can install JAX and its associated libraries using pip:\n",
        "\n",
        "```bash\n",
        "pip install jax jaxlib\n",
        "```\n",
        "\n",
        "For CPU-only usage, this is sufficient.  To utilize GPUs or TPUs, you'll need to install the appropriate JAXlib backend for your hardware.  Refer to the official JAX documentation ([https://jax.readthedocs.io/en/latest/](https://www.google.com/url?sa=E&source=gmail&q=https://jax.readthedocs.io/en/latest/)) for detailed instructions on setting up GPU and TPU acceleration.\n",
        "\n",
        "For most of the examples in this book, CPU execution will be adequate to demonstrate the core concepts and algorithm implementations. However, for computationally intensive tasks or larger datasets, leveraging GPUs or TPUs with JAX will become essential, and we'll highlight where these performance gains become particularly noticeable.\n",
        "\n",
        "## 1.5 A Taste of JAX in Action: Linear Regression\n",
        "\n",
        "To give you a quick taste of how these concepts come together, let's consider a very simple example: linear regression using gradient descent in JAX.  We won't go into detail about linear regression theory here (we'll cover that in depth later), but this will demonstrate the power of JAX."
      ],
      "metadata": {
        "id": "OvzN4GBRBJGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax # A popular JAX optimization library\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "key = jax.random.PRNGKey(0)\n",
        "key, subkey = jax.random.split(key)\n",
        "X = jax.random.uniform(subkey, (100, 1)) * 10 # Features\n",
        "y_true = 2 * X + 1 + jax.random.normal(key, (100, 1)) # True relationship + noise\n",
        "\n",
        "# 2. Define the model (linear regression)\n",
        "def predict(params, x):\n",
        "    w, b = params\n",
        "    return w * x + b\n",
        "\n",
        "# 3. Define the loss function (Mean Squared Error)\n",
        "def loss_fn(params, x, y):\n",
        "    predictions = predict(params, x)\n",
        "    return jnp.mean((predictions - y)**2)\n",
        "\n",
        "# 4. Initialize parameters\n",
        "params = (jnp.array([0.0]), jnp.array([0.0])) # Initial guess: w=0, b=0\n",
        "\n",
        "# 5. Define the optimizer (using Optax - Gradient Descent)\n",
        "optimizer = optax.sgd(learning_rate=0.01)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "# 6. Training loop\n",
        "@jax.jit # Compile the training step for speed\n",
        "def train_step(params, opt_state, x_batch, y_batch):\n",
        "    grads = jax.grad(loss_fn)(params, x_batch, y_batch) # Compute gradients using jax.grad\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params) # Get updates from optimizer\n",
        "    params = optax.apply_updates(params, updates) # Apply updates to parameters\n",
        "    return params, opt_state\n",
        "\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    params, opt_state = train_step(params, opt_state, X, y_true)\n",
        "\n",
        "# 7. Learned parameters\n",
        "learned_w, learned_b = params\n",
        "print(f\"Learned weight (w): {learned_w[0]:.4f}, Learned bias (b): {learned_b[0]:.4f}\")\n",
        "print(f\"True weight: 2.0, True bias: 1.0\") # Compare to the true values we used to generate data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "q3-MD_fVBJGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even in this simple example, you can see how JAX comes into play:\n",
        "\n",
        "  * **`jax.numpy`:** Used for array operations, data generation, and calculations.\n",
        "  * **`jax.jit`:**  Compiling the `train_step` function for faster training iterations.\n",
        "  * **`jax.grad`:**  Automatically calculating the gradients of the `loss_fn`, essential for gradient descent.\n",
        "  * **Optax:**  While we used an external optimization library (Optax) here for convenience, you could also implement the gradient descent update rule entirely from scratch in JAX, leveraging `jax.grad`.\n",
        "\n",
        "This is just a glimpse of what JAX can do.  As we progress through this book, you'll see how these core concepts and JAX's powerful capabilities enable us to implement a wide range of data science algorithms with efficiency and clarity.\n",
        "\n",
        "## 1.6 What to Expect in This Book\n",
        "\n",
        "In the chapters that follow, we will systematically explore common data science algorithms across statistics, econometrics, experimentation, and machine learning. For each algorithm, we will:\n",
        "\n",
        "  * **Understand the Theoretical Foundations:**  We'll delve into the underlying mathematical principles and statistical concepts.\n",
        "  * **Develop JAX Implementations from Scratch (Where Possible):**  We'll implement the algorithms in JAX, often from the ground up, to gain a deep understanding of their inner workings and how to translate theory into code.\n",
        "  * **Utilize JAX's Features for Efficiency and Clarity:**  We'll leverage `jax.numpy`, `jax.jit`, `jax.grad`, `jax.vmap`, and other JAX functionalities to create performant and readable implementations.\n",
        "  * **Provide Practical Examples and Code:**  Each chapter will include runnable JAX code examples to illustrate the algorithms in action.\n",
        "\n",
        "By the end of this book, you'll not only have a solid understanding of these essential data science algorithms but also the practical skills to implement and apply them effectively using JAX.\n",
        "\n",
        "Let's begin our journey in the next chapter by diving into the world of descriptive statistics with JAX\\!\n"
      ],
      "metadata": {
        "id": "8Fk-sb28BJGs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t596qWhBCrOu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}